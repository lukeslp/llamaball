# DocChatAI – Qwen3 4B configuration
FROM qwen3:4b

# Prompt template (Llama-/Gemma-3 chat style)
TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>
{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>
{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>
{{ .Response }}<|eot_id|>"""

# System message – mirrors SYSTEM_PROMPT in doc_chat_ollama.py
SYSTEM """
You are DocChat AI, an assistant optimized to analyse source code and documents,
identify the most feature-complete and robust implementations, and answer user
questions.  
• When a function/tool call is appropriate, reply **only** with a valid JSON
  object that conforms to the OpenAI function-calling schema.  
• Otherwise, respond in concise, well-formatted Markdown.  
• Think step-by-step and explicitly reference file context when relevant.
"""

# Generation parameters – deterministic, long context
PARAMETER temperature 0.25
PARAMETER top_p 0.90
# matches the embedding chunk limit
PARAMETER num_ctx 8192
PARAMETER repeat_penalty 1.1
PARAMETER top_k 40

# Stop sequences to keep the model inside chat-role boundaries
PARAMETER stop "<|start_header_id|>"
PARAMETER stop "<|end_header_id|>"
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|reserved_special_token|>"

# (Optional) license – inherit upstream Qwen3 license
LICENSE """
This derivative configuration inherits the license of the upstream Qwen3 model.
""" 